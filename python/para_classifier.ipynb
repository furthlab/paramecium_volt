{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from ppca import PPCA\n",
    "from sklearn.model_selection import GridSearchCV, ShuffleSplit, cross_val_score\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set variables\n",
    "\n",
    "filepath = '../processed'\n",
    "fps = 30                    # frames per second\n",
    "baseline = 2                # pre-stimulus baseline (secs)\n",
    "cs_dur = 4                  # CS duration (secs); note that last 2 secs coincide with shock\n",
    "cr_dur = 2                  # Length of CS period to measure CR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(behavior, timestamps):\n",
    "    \n",
    "    # Step 1: Filter for \"CS\" \"on\" events\n",
    "    cs_on_events = timestamps[(timestamps['stimulus'] == 'CS') & (timestamps['event'] == 'on')]\n",
    "\n",
    "    # Step 2: Identify frames where stimulus is \"US\" and event is \"on\"\n",
    "    us_on_frames = timestamps[(timestamps['stimulus'] == 'US') & (timestamps['event'] == 'on')]['frame'].values\n",
    "    \n",
    "    # Function to get sequences of frames\n",
    "    def get_sequences(start_frames, offset):\n",
    "        sequences = []\n",
    "        for frame in start_frames:\n",
    "            if offset == 1:\n",
    "                sequence_frames = range(frame, frame + 60)  # Frames after the event\n",
    "            elif offset == -1:\n",
    "                sequence_frames = range(frame - 60, frame)  # Frames before the event\n",
    "\n",
    "            # Exclude frames where stimulus is \"US\" \"on\"\n",
    "            valid_frames = [f for f in sequence_frames if f not in us_on_frames]\n",
    "\n",
    "            # Get dx and dy values for valid frames\n",
    "            valid_behavior = behavior[behavior['frame'].isin(valid_frames)]\n",
    "\n",
    "            # Check if we have enough frames (should be 60)\n",
    "            if len(valid_behavior) >= 60:\n",
    "                # Take the first 60 frames (in case we excluded some)\n",
    "                valid_behavior = valid_behavior.head(60) if offset == 1 else valid_behavior.tail(60)\n",
    "                # Concatenate dx and dy values\n",
    "                sequence = np.hstack((valid_behavior['dx'].values, valid_behavior['dy'].values))\n",
    "                sequences.append(sequence)\n",
    "        \n",
    "        return sequences\n",
    "\n",
    "    # Step 3: Collect the sequences after and before each \"CS\" \"on\" event\n",
    "    sequences_after_cs = get_sequences(cs_on_events['frame'], 1)\n",
    "    sequences_before_cs = get_sequences(cs_on_events['frame'], -1)\n",
    "\n",
    "    # Step 4: Create numpy arrays and stack them\n",
    "    result_array_after = np.array(sequences_after_cs)\n",
    "    result_array_before = np.array(sequences_before_cs)\n",
    "    features = np.vstack((result_array_after, result_array_before))\n",
    "\n",
    "    # Step 5: Create labels array\n",
    "    labels_after = np.ones(result_array_after.shape[0])\n",
    "    labels_before = np.zeros(result_array_before.shape[0])\n",
    "    labels = np.hstack((labels_after, labels_before))\n",
    "    \n",
    "    return features, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "\n",
    "def load_data(filepath):\n",
    "    \n",
    "    Features = np.empty((0,120))\n",
    "    Labels = np.empty((0,1))\n",
    "    for f in os.listdir(filepath):\n",
    "        if f.endswith(\"_timestamps.csv\"):\n",
    "            # Load timestamps\n",
    "            timestamps = pd.read_csv(os.path.join(filepath,f))\n",
    "            timestamps['event'] = timestamps['event'].apply(lambda x: x.lower() if isinstance(x, str) else x) # get rid of capitalization\n",
    "            \n",
    "            # Load behavior\n",
    "            p = f.split(\"_\")\n",
    "            g = \"_\".join([p[0],p[1],'behavior.csv'])\n",
    "            behavior = pd.read_csv(os.path.join(filepath,g))\n",
    "            behavior['dx'] = behavior.groupby('ID')['x'].diff()\n",
    "            behavior['dy'] = behavior.groupby('ID')['y'].diff()\n",
    "            behavior.fillna(0)\n",
    "            \n",
    "            features, labels = extract_features(behavior,timestamps)\n",
    "            \n",
    "            Features = np.append(Features,features,axis=0)\n",
    "            Labels = np.append(Labels,labels)\n",
    "            \n",
    "    return Features, Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data and extract features\n",
    "Features, Labels = load_data(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce dimensionality using probabilistic PCA\n",
    "Features = StandardScaler().fit_transform(Features)     # Z-score first\n",
    "ppca = PPCA()\n",
    "ppca.fit(data=Features, d=20, verbose=False)\n",
    "Components = ppca.transform()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.48837209, 0.50581395, 0.53488372, 0.47674419, 0.48255814])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Classify trials as US vs. ITI\n",
    "clf = SVC(kernel='linear', C=1, random_state=42)\n",
    "scores = cross_val_score(clf, Components, Labels, cv=5)\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best parameters are {'C': 1000000.0, 'gamma': 0.1} with a score of 0.55\n"
     ]
    }
   ],
   "source": [
    "C_range = np.logspace(-2, 10, 4)\n",
    "gamma_range = np.logspace(-9, 3, 4)\n",
    "param_grid = dict(gamma=gamma_range, C=C_range)\n",
    "cv = ShuffleSplit(n_splits=5, test_size=0.2, random_state=42)\n",
    "grid = GridSearchCV(SVC(kernel='rbf'), param_grid=param_grid, cv=cv)\n",
    "grid.fit(Components, Labels)\n",
    "\n",
    "print(\n",
    "    \"The best parameters are %s with a score of %0.2f\"\n",
    "    % (grid.best_params_, grid.best_score_)\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
